# Overview
## Specific Aims
This paper aims to predict housing sale prices using machine learning in San Francisco. The hypothesis states that the machine learning models will outperform simple regression models for the same given dataset. Furthermore, the hypothesis states that feature additions including crime, school, and transportation will lead to better predictions. The San Francisco dataset is used as a preliminary example for the UAE dataset. The aim of the overall capstone project is to build a series of machine learning models to better predict housing sale prices. This will include a data preparation library for fine tuning inputs to the provided models. The models will be adaptable to both San Francisco and the UAE and thus would assumedly generalize to most city housing datasets.

## Background
Predicting housing prices has become a popular introductory example for machine learning models. This was popularized with the Boston Housing Data, readily available on many open source machine learning libraries. Furthermore, in economics predicting housing prices has been classically performed with a variety of models including but not limited to: the linear asset pricing, hedonistic regression, repeat sales model. These models are effective at identifying multi-variate relationships between the input features and the output sale price. However, they often fail to truly predict sale prices at a degree of accuracy that is meaningful for market actors. For this reason, most market actors reply on complex machine learning models to accurately price property. For example, Zillow, a popular US-based property data platforms, provides a Z-estimate of each house listed on its platform. These Z-estimates estimate the current value of the home. According to Zillow research, the Z-estimate combines several million machine learning models together to generate these values. Furthermore, the Zillow research team tests over 65,000 new machine learning models each day to improve upon their prediction. [https://www.zillow.com/tech/introducing-a-new-and-improved-zestimate-algorithm/] The Z-estimate is highly accurate according to a Zillow blog post from Chief Analytics Officer Stan Humphries which states, "[the] Zestimate now has a median error rate of less than 2 percent for homes listed for sale, meaning half of all Zestimates fall within 2 percent of the homeâ€™s eventual sales price." This sets the benchmark for what is possible in the space of predicting housing prices. Zillow, however, has access to highly accurate and varied datasets across the entire United States. Nevertheless, such a prediction model does not exist for the UAE. The outcome of this entire project is to build a model with the median error rate of less than 5 percent in the UAE. The processes employed for building such a model have been conducted with San Francisco data. This aligns with the eventual goal of adapting the model to the UAE, once the data has been acquired.

## Data
The data for San Francisco sales was collected from the U.S. National Realtors Association. The dataset includes all single-family home sales between 2007 and 2017.
### Ideal Dataset
The ideal data set includes all housing sales in a recent time frame. Within the dataset the column space includes: hedonistic features (number of rooms, number of bath, square footage, number of bedrooms, etc.), location data (latitude, longitude, neighborhood, zip code, etc.), important features (number of crimes in the neighborhood, school ratings in the neighborhood, number of blocks to public transportation, etc.)

### Neighborhoods and Spatial Join
There are 104 realtor neighborhoods that were represented in the dataset. These neighborhoods are used by real estate regions to subdivide the zip coded areas of San Francisco. Thus, the paper assumes that these neighborhoods encode a non-linear transformation of the location data in San Francisco. For this reason, the neighborhoods were added to each housing sale in the dataset. Furthermore, schooling and crime data was spatially joined to the dataset by aggregating on each neighborhood. This technique was employed for both ease and accuracy. However, better methods are possible and could be employed in future iterations of the project. One possible extension could be approximating the density of the given indicator at each location of the housing sale using GPU programming.

### Great Schools API & Data
Schooling data is a known indicator of housing prices in the US. This is often due to the nature of US public school zoning laws and large discrepancies in the quality of public school between neighborhoods. Thus, housing prices can be deeply influenced by school data. To incorporate this type of data in my models, I utilized Great Schools API to get data on all schools in San Francisco. The data included the location (latitude and longitude), a great schools rating (made by Great Schools themselves, explanation can be found here https://www.greatschools.org/gk/ratings/), the number of public, private, and charter schools, the number of enrolled students, and the public school district the school was located in. Some of the data was missing or incomplete. About 40% of the schools did not have a Great schools rating. About 25% did not have the number of enrolled students. The average value for each respective indicator was used to fill the missing columns. This was the technique employed for its ease, further discussion can be found in the filling empty data section. There was 32 neighborhoods that did not have any school data and thus about 3000 housing sales that did not have any school data.

#### Spatial Binning of School Data
The school data was spatially binned by realtor neighborhoods using a GeoJson file of San Francisco realtor neighborhoods provided by OpenSF [https://data.sfgov.org/Geographic-Locations-and-Boundaries/SF-Find-Neighborhoods/pty2-tcw4]. The average enrollment and Great Schools rating and the total number of private, public, and charter schools per neighborhood were generated. This data can be viewed in the repository under "ENG_DATA/CLEANED/school-data-by-neighborhood.csv".

### Crime Data
Crime is another known indicator of housing prices in the US. One can imagine that houses in relatively safe areas will be more likely to be higher priced than houses in relatively unsafe areas. To test this assumption and include such indicator in the models, the San Francisco police crime data set was gathered. The data set is publicly available at [https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-Historical-2003/tmnf-yvry]. The data contains all crimes from 2003 to 2018. This is over 2 million rows of individual crime data. A 20 percent random sample of the entire dataset was used for simplicity and easier processing.

#### Spatial Binning of Crime Data
The crime data was spatially binned by realtor neighborhoods using a GeoJson file of San Francisco realtor neighborhoods provided by OpenSF [https://data.sfgov.org/Geographic-Locations-and-Boundaries/SF-Find-Neighborhoods/pty2-tcw4].  
The crime data included over 50 different types of crimes. To distinguish between crimes, the number of incidents of each type of crime was included in the spatially binned dataset in addition to the total number of incidents per neighborhood. The data can be viewed in the repository under "ENG_DATA/CLEANED/crime-data-by-neighborhood.csv".

## Filling Empty Data
Empty data was filled using the average value for the given indicator. This is a common technique. However, it often leads to inaccuracies and incorrect assumptions in the dataset. A better methodology, which could be employed in the next iteration, would be to build a prior distribution on the empty data. This could be done by training a bayesian model to predict the empty data indicator. Then, when an empty piece of data is encountered, the trained distribution's MAP or MLE on the empty data column for the given data row is used.

### Feature Visualization
The feature visualization has a few specific outcomes: to understand univariate and spatial distributions of features in the dataset and to test mutual information and correlation coefficients between features and the target variable. This will help better inform model outcomes and important features for predicting the sale price. This will also help detect outliers. Lastly, this will help decide on the removal of certain features from the dataset in the feature selection section. Low variability and low mutual information of features often results in little impact of a feature on the predicted target variable. Presentation of the feature visualization and specific learnings for the San Francisco dataset can be found in the results section.

### Feature Encoding
Feature encoding is vital for processing categorical or ordinal data in the dataset. Two common techniques are one-hot encoding and label encoding. Label encoding creates adds an inherent weight to the categorical data, while one-hot encoding does not. One-hot encoding results in high dimensionality and is often used in conjunction with PCA or another dimensionality reduction technique. Label-encoding does not result in high dimensionality but will only yield results for Random Forests and Decision Trees. For other methods, it fails as the data is, as mentioned, ranked. For this reason, one-hot encoding with PCA was used for the linear classifier, while both one-hot encoding with PCA and label encoding was used for the machine learning models. It is important to note, that AutoML handles categorical data with a unique variety of methods. This is part of its efficacy as an out-of-the-box methodology.

### Feature Selection
Feature selection is the process of reducing the space of dimensionality in the dataset before training the models. It often results in faster training and reduces the VC dimensionality of the target set. For this reason, it is optimal for large or sparse datasets with high dimensionality. This is another extension, that the entire project hopes to capitalize on when a larger dataset is obtained. As discussed in the results section, for the San Francisco dataset, feature selection was avoided. This was due to poor performance in the preliminary machine learning models after feature selection. Perhaps, a more in-depth understanding of how to select features was required to accurately select features without reducing model accuracy.

## Methodology
The methodology begins with research and collection of the ideal dataset, specified in the `Data` section under the `Ideal Dataset`. This type of dataset was unattainable within semester's timeline. For this reason, the San Francisco dataset and example has been used. Once the dataset is collected, the first step is to understand and clean the dataset. This may involve filling missing data, finding missing data, identifying outliers, removing outliers. The next step is to pull in important features. This is heavily bias to the city in focus. For the US, crime and school quality are two crucial aspects to understanding housing prices. In Singapore, proximity to public transportation is crucial to understanding housing prices. In Abu Dhabi and Dubai, proximity to Sheik Zayed Road may be critical to housing prices. Next, visualizing the dataset is performed. Reference to methodology can be found under the `Data` and `Feature Visualization` section. Visualization is important for fine tuning models and understanding failure. Finally, feature encoding is performed to allow categorical data to be interpreted by the models used. Feature selection is optionally performed to reduce the dimensionality of the data space. This is critical for one-hot encoded data as the dimensionality can grow exponentially. This can prove ruinous for the models accuracy.
Once data cleaning is complete, the dataset is split by training, validation, and test data. The split is generally: 0.7, 0.1, 0.2, respectively. The models ingest the training data, perform cross-validation on the validation data, and output the score based on the test data.
Finally, model performance is conducted to evaluate the results and impact. R-squared values are used as a primary indicator for model performance as the target variable is continuous. Percent test error is also used to identify outliers in the test error. Finally, percent test error is visualized spatially across the city.

## Hypotheses
There are two primary hypothesis to be evaluated. The first hypothesis states that the machine learning models will outperform simple regression models for the same given dataset. These machine learning models are specified to be the random forest model (extremely randomized trees) and the leading AutoML model (which is an ensemble technique). The second hypothesis states that feature additions to the datasets including crime, school, and transportation indicators will lead to better predictions in the machine learning models.

# Results
## H20 Models
The AutoML H20 models were used to evaluate the data collected. The models set a benchmark for the best in class predictors in out of the box machine learning. The top AutoML model was an ensemble method which compiles the best individual regressors (Decision Trees, Neural Networks, Random Forests, etc...). The model had a cross-validation R squared of 0.81. The test R squared was 0.79.
